{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2118595,"sourceType":"datasetVersion","datasetId":1271215},{"sourceId":14452219,"sourceType":"datasetVersion","datasetId":9230606}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile voc.yaml\ndataset_params:\n  # Train/Val split is used for both training and testing here\n  im_train_path: '/kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/JPEGImages'\n  ann_train_path: '/kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/Annotations'\n  im_test_path: '/kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/JPEGImages'\n  ann_test_path: '/kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/Annotations'\n  num_classes : 21\n\nmodel_params:\n  im_channels : 3\n  aspect_ratios: [0.5, 1, 2]\n  scales: [128, 256, 512]\n  min_im_size : 600\n  max_im_size : 1000\n  backbone_out_channels : 512\n  fc_inner_dim : 1024\n  rpn_bg_threshold : 0.3\n  rpn_fg_threshold : 0.7\n  rpn_nms_threshold : 0.7\n  rpn_train_prenms_topk : 12000\n  rpn_test_prenms_topk : 6000\n  rpn_train_topk : 2000\n  rpn_test_topk : 300\n  rpn_batch_size : 256\n  rpn_pos_fraction : 0.5\n  roi_iou_threshold : 0.5\n  roi_low_bg_iou : 0.0\n  roi_pool_size : 7\n  roi_nms_threshold : 0.3\n  roi_topk_detections : 100\n  roi_score_threshold : 0.05\n  roi_batch_size : 128\n  roi_pos_fraction : 0.25\n\ntrain_params:\n  task_name: 'voc_scratch_kaggle'\n  seed : 1111\n  acc_steps : 4\n  num_epochs: 50\n  lr_steps : [35, 45]\n  lr: 0.0001\n  ckpt_name: '/kaggle/input/voc-scratch-kaggle-2/faster_rcnn_voc_scratch.pth'","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ok8AHGLnX1hv","outputId":"a4e6d41b-13bc-4ad1-a9b8-972cb8625c6c","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:25:00.697266Z","iopub.execute_input":"2026-01-10T07:25:00.698037Z","iopub.status.idle":"2026-01-10T07:25:00.703555Z","shell.execute_reply.started":"2026-01-10T07:25:00.698008Z","shell.execute_reply":"2026-01-10T07:25:00.702878Z"}},"outputs":[{"name":"stdout","text":"Overwriting voc.yaml\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"%%writefile voc.py\nimport glob\nimport os\nimport random\n\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data.dataset import Dataset\nimport xml.etree.ElementTree as ET\n\n\ndef load_images_and_anns(im_dir, ann_dir, label2idx):\n    r\"\"\"\n    Method to get the xml files and for each file\n    get all the objects and their ground truth detection\n    information for the dataset\n    :param im_dir: Path of the images\n    :param ann_dir: Path of annotation xmlfiles\n    :param label2idx: Class Name to index mapping for dataset\n    :return:\n    \"\"\"\n    im_infos = []\n    for ann_file in tqdm(glob.glob(os.path.join(ann_dir, '*.xml'))):\n        im_info = {}\n        im_info['img_id'] = os.path.basename(ann_file).split('.xml')[0]\n        im_info['filename'] = os.path.join(im_dir, '{}.jpg'.format(im_info['img_id']))\n        ann_info = ET.parse(ann_file)\n        root = ann_info.getroot()\n        size = root.find('size')\n        width = int(size.find('width').text)\n        height = int(size.find('height').text)\n        im_info['width'] = width\n        im_info['height'] = height\n        detections = []\n\n        for obj in ann_info.findall('object'):\n            det = {}\n            label = label2idx[obj.find('name').text]\n            bbox_info = obj.find('bndbox')\n            bbox = [\n                int(float(bbox_info.find('xmin').text))-1,\n                int(float(bbox_info.find('ymin').text))-1,\n                int(float(bbox_info.find('xmax').text))-1,\n                int(float(bbox_info.find('ymax').text))-1\n            ]\n            det['label'] = label\n            det['bbox'] = bbox\n            detections.append(det)\n        im_info['detections'] = detections\n        im_infos.append(im_info)\n    print('Total {} images found'.format(len(im_infos)))\n    return im_infos\n\n\nclass VOCDataset(Dataset):\n    def __init__(self, split, im_dir, ann_dir):\n        self.split = split\n        self.im_dir = im_dir\n        self.ann_dir = ann_dir\n        classes = [\n            'person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n            'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n            'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor'\n        ]\n        classes = sorted(classes)\n        classes = ['background'] + classes\n        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n        print(self.idx2label)\n        self.images_info = load_images_and_anns(im_dir, ann_dir, self.label2idx)\n\n    def __len__(self):\n        return len(self.images_info)\n\n    def __getitem__(self, index):\n        im_info = self.images_info[index]\n        im = Image.open(im_info['filename'])\n        to_flip = False\n        if self.split == 'train' and random.random() < 0.5:\n            to_flip = True\n            im = im.transpose(Image.FLIP_LEFT_RIGHT)\n        im_tensor = torchvision.transforms.ToTensor()(im)\n        targets = {}\n        targets['bboxes'] = torch.as_tensor([detection['bbox'] for detection in im_info['detections']])\n        targets['labels'] = torch.as_tensor([detection['label'] for detection in im_info['detections']])\n        if to_flip:\n            for idx, box in enumerate(targets['bboxes']):\n                x1, y1, x2, y2 = box\n                w = x2-x1\n                im_w = im_tensor.shape[-1]\n                x1 = im_w - x1 - w\n                x2 = x1 + w\n                targets['bboxes'][idx] = torch.as_tensor([x1, y1, x2, y2])\n        return im_tensor, targets, im_info['filename']","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OyYxVk9CaO-i","outputId":"e11ef24b-dc62-416a-fe46-6035db9d544f","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:25:04.874449Z","iopub.execute_input":"2026-01-10T07:25:04.874969Z","iopub.status.idle":"2026-01-10T07:25:04.881108Z","shell.execute_reply.started":"2026-01-10T07:25:04.874947Z","shell.execute_reply":"2026-01-10T07:25:04.880279Z"}},"outputs":[{"name":"stdout","text":"Overwriting voc.py\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"%%writefile faster_rcnn.py\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport math\n\ndef get_iou(boxes1, boxes2):\n    r\"\"\"\n    IOU between two sets of boxes\n    :param boxes1: (Tensor of shape N x 4)\n    :param boxes2: (Tensor of shape M x 4)\n    :return: IOU matrix of shape N x M\n    \"\"\"\n    # Area of boxes (x2-x1)*(y2-y1)\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # (N,)\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # (M,)\n\n    # Get top left x1,y1 coordinate\n    x_left = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # (N, M)\n    y_top = torch.max(boxes1[:, None, 1], boxes2[:, 1])  # (N, M)\n\n    # Get bottom right x2,y2 coordinate\n    x_right = torch.min(boxes1[:, None, 2], boxes2[:, 2])  # (N, M)\n    y_bottom = torch.min(boxes1[:, None, 3], boxes2[:, 3])  # (N, M)\n\n    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)  # (N, M)\n    union = area1[:, None] + area2 - intersection_area  # (N, M)\n    iou = intersection_area / union  # (N, M)\n    return iou\n\n\ndef boxes_to_transformation_targets(ground_truth_boxes, anchors_or_proposals):\n    r\"\"\"\n    Given all anchor boxes or proposals in image and their respective\n    ground truth assignments, we use the x1,y1,x2,y2 coordinates of them\n    to get tx,ty,tw,th transformation targets for all anchor boxes or proposals\n    \"\"\"\n    # Get center_x,center_y,w,h from x1,y1,x2,y2 for anchors\n    widths = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n    heights = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n    center_x = anchors_or_proposals[:, 0] + 0.5 * widths\n    center_y = anchors_or_proposals[:, 1] + 0.5 * heights\n\n    # Get center_x,center_y,w,h from x1,y1,x2,y2 for gt boxes\n    gt_widths = ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n    gt_heights = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n    gt_center_x = ground_truth_boxes[:, 0] + 0.5 * gt_widths\n    gt_center_y = ground_truth_boxes[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_center_x - center_x) / widths\n    targets_dy = (gt_center_y - center_y) / heights\n    targets_dw = torch.log(gt_widths / widths)\n    targets_dh = torch.log(gt_heights / heights)\n    regression_targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n    return regression_targets\n\n\ndef apply_regression_pred_to_anchors_or_proposals(box_transform_pred, anchors_or_proposals):\n    r\"\"\"\n    Given the transformation parameter predictions for all\n    input anchors or proposals, transform them accordingly\n    to generate predicted proposals or predicted boxes\n    \"\"\"\n    box_transform_pred = box_transform_pred.reshape(\n        box_transform_pred.size(0), -1, 4)\n\n    # Get cx, cy, w, h from x1,y1,x2,y2\n    w = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n    h = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n    center_x = anchors_or_proposals[:, 0] + 0.5 * w\n    center_y = anchors_or_proposals[:, 1] + 0.5 * h\n\n    dx = box_transform_pred[..., 0]\n    dy = box_transform_pred[..., 1]\n    dw = box_transform_pred[..., 2]\n    dh = box_transform_pred[..., 3]\n\n    # Prevent sending too large values into torch.exp()\n    dw = torch.clamp(dw, max=math.log(1000.0 / 16))\n    dh = torch.clamp(dh, max=math.log(1000.0 / 16))\n\n    pred_center_x = dx * w[:, None] + center_x[:, None]\n    pred_center_y = dy * h[:, None] + center_y[:, None]\n    pred_w = torch.exp(dw) * w[:, None]\n    pred_h = torch.exp(dh) * h[:, None]\n\n    pred_box_x1 = pred_center_x - 0.5 * pred_w\n    pred_box_y1 = pred_center_y - 0.5 * pred_h\n    pred_box_x2 = pred_center_x + 0.5 * pred_w\n    pred_box_y2 = pred_center_y + 0.5 * pred_h\n\n    pred_boxes = torch.stack((\n        pred_box_x1,\n        pred_box_y1,\n        pred_box_x2,\n        pred_box_y2),\n        dim=2)\n    return pred_boxes\n\n\ndef sample_positive_negative(labels, positive_count, total_count):\n    # Sample positive and negative proposals\n    positive = torch.where(labels >= 1)[0]\n    negative = torch.where(labels == 0)[0]\n    num_pos = positive_count\n    num_pos = min(positive.numel(), num_pos)\n    num_neg = total_count - num_pos\n    num_neg = min(negative.numel(), num_neg)\n    perm_positive_idxs = torch.randperm(positive.numel(),\n                                        device=positive.device)[:num_pos]\n    perm_negative_idxs = torch.randperm(negative.numel(),\n                                        device=negative.device)[:num_neg]\n    pos_idxs = positive[perm_positive_idxs]\n    neg_idxs = negative[perm_negative_idxs]\n    sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n    sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n    sampled_pos_idx_mask[pos_idxs] = True\n    sampled_neg_idx_mask[neg_idxs] = True\n    return sampled_neg_idx_mask, sampled_pos_idx_mask\n\n\ndef clamp_boxes_to_image_boundary(boxes, image_shape):\n    boxes_x1 = boxes[..., 0]\n    boxes_y1 = boxes[..., 1]\n    boxes_x2 = boxes[..., 2]\n    boxes_y2 = boxes[..., 3]\n    height, width = image_shape[-2:]\n    boxes_x1 = boxes_x1.clamp(min=0, max=width)\n    boxes_x2 = boxes_x2.clamp(min=0, max=width)\n    boxes_y1 = boxes_y1.clamp(min=0, max=height)\n    boxes_y2 = boxes_y2.clamp(min=0, max=height)\n    boxes = torch.cat((\n        boxes_x1[..., None],\n        boxes_y1[..., None],\n        boxes_x2[..., None],\n        boxes_y2[..., None]),\n        dim=-1)\n    return boxes\n\n\ndef transform_boxes_to_original_size(boxes, new_size, original_size):\n    r\"\"\"\n    Boxes are for resized image (min_size=600, max_size=1000).\n    This method converts the boxes to whatever dimensions\n    the image was before resizing\n    \"\"\"\n    ratios = [\n        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n        / torch.tensor(s, dtype=torch.float32, device=boxes.device)\n        for s, s_orig in zip(new_size, original_size)\n    ]\n    ratio_height, ratio_width = ratios\n    xmin, ymin, xmax, ymax = boxes.unbind(1)\n    xmin = xmin * ratio_width\n    xmax = xmax * ratio_width\n    ymin = ymin * ratio_height\n    ymax = ymax * ratio_height\n    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\n\n\nclass RegionProposalNetwork(nn.Module):\n    r\"\"\"\n    RPN with following layers on the feature map\n        1. 3x3 conv layer followed by Relu\n        2. 1x1 classification conv with num_anchors output channels\n        3. 1x1 classification conv with 4 x num_anchors output channels\n    \"\"\"\n\n    def __init__(self, in_channels, scales, aspect_ratios, model_config):\n        super(RegionProposalNetwork, self).__init__()\n        self.scales = scales\n        self.low_iou_threshold = model_config['rpn_bg_threshold']\n        self.high_iou_threshold = model_config['rpn_fg_threshold']\n        self.rpn_nms_threshold = model_config['rpn_nms_threshold']\n        self.rpn_batch_size = model_config['rpn_batch_size']\n        self.rpn_pos_count = int(model_config['rpn_pos_fraction'] * self.rpn_batch_size)\n        self.rpn_topk = model_config['rpn_train_topk'] if self.training else model_config['rpn_test_topk']\n        self.rpn_prenms_topk = model_config['rpn_train_prenms_topk'] if self.training \\\n            else model_config['rpn_test_prenms_topk']\n        self.aspect_ratios = aspect_ratios\n        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n\n        # 3x3 conv layer\n        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n\n        # 1x1 classification conv layer\n        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n\n        # 1x1 regression\n        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n\n        for layer in [self.rpn_conv, self.cls_layer, self.bbox_reg_layer]:\n            torch.nn.init.normal_(layer.weight, std=0.01)\n            torch.nn.init.constant_(layer.bias, 0)\n\n    def generate_anchors(self, image, feat):\n        r\"\"\"\n        Method to generate anchors.\n        \"\"\"\n        grid_h, grid_w = feat.shape[-2:]\n        image_h, image_w = image.shape[-2:]\n\n        # For the vgg16 case stride would be 16 for both h and w\n        stride_h = torch.tensor(image_h // grid_h, dtype=torch.int64, device=feat.device)\n        stride_w = torch.tensor(image_w // grid_w, dtype=torch.int64, device=feat.device)\n\n        scales = torch.as_tensor(self.scales, dtype=feat.dtype, device=feat.device)\n        aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n\n        h_ratios = torch.sqrt(aspect_ratios)\n        w_ratios = 1 / h_ratios\n\n        ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n        hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n\n        # Base anchors (zero-centered)\n        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n        base_anchors = base_anchors.round()\n\n        # Shifts\n        shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n        shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n\n        shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n\n        shifts_x = shifts_x.reshape(-1)\n        shifts_y = shifts_y.reshape(-1)\n        shifts = torch.stack((shifts_x, shifts_y, shifts_x, shifts_y), dim=1)\n\n        anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4))\n        anchors = anchors.reshape(-1, 4)\n        return anchors\n\n    def assign_targets_to_anchors(self, anchors, gt_boxes):\n        # Get (gt_boxes, num_anchors_in_image) IOU matrix\n        iou_matrix = get_iou(gt_boxes, anchors)\n\n        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n        best_match_gt_idx_pre_thresholding = best_match_gt_idx.clone()\n\n        below_low_threshold = best_match_iou < self.low_iou_threshold\n        between_thresholds = (best_match_iou >= self.low_iou_threshold) & (best_match_iou < self.high_iou_threshold)\n        best_match_gt_idx[below_low_threshold] = -1\n        best_match_gt_idx[between_thresholds] = -2\n\n        # Ensure every GT box has at least one anchor\n        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n        gt_pred_pair_with_highest_iou = torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n        pred_inds_to_update = gt_pred_pair_with_highest_iou[1]\n        best_match_gt_idx[pred_inds_to_update] = best_match_gt_idx_pre_thresholding[pred_inds_to_update]\n\n        matched_gt_boxes = gt_boxes[best_match_gt_idx.clamp(min=0)]\n\n        labels = best_match_gt_idx >= 0\n        labels = labels.to(dtype=torch.float32)\n\n        background_anchors = best_match_gt_idx == -1\n        labels[background_anchors] = 0.0\n\n        ignored_anchors = best_match_gt_idx == -2\n        labels[ignored_anchors] = -1.0\n\n        return labels, matched_gt_boxes\n\n    def filter_proposals(self, proposals, cls_scores, image_shape):\n        cls_scores = cls_scores.reshape(-1)\n        cls_scores = torch.sigmoid(cls_scores)\n        _, top_n_idx = cls_scores.topk(min(self.rpn_prenms_topk, len(cls_scores)))\n\n        cls_scores = cls_scores[top_n_idx]\n        proposals = proposals[top_n_idx]\n\n        proposals = clamp_boxes_to_image_boundary(proposals, image_shape)\n\n        min_size = 16\n        ws, hs = proposals[:, 2] - proposals[:, 0], proposals[:, 3] - proposals[:, 1]\n        keep = (ws >= min_size) & (hs >= min_size)\n        keep = torch.where(keep)[0]\n        proposals = proposals[keep]\n        cls_scores = cls_scores[keep]\n\n        keep_mask = torch.zeros_like(cls_scores, dtype=torch.bool)\n        keep_indices = torch.ops.torchvision.nms(proposals, cls_scores, self.rpn_nms_threshold)\n        keep_mask[keep_indices] = True\n        keep_indices = torch.where(keep_mask)[0]\n\n        post_nms_keep_indices = keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n\n        proposals, cls_scores = (proposals[post_nms_keep_indices[:self.rpn_topk]],\n                                 cls_scores[post_nms_keep_indices[:self.rpn_topk]])\n\n        return proposals, cls_scores\n\n    def forward(self, image, feat, target=None):\n        rpn_feat = nn.ReLU()(self.rpn_conv(feat))\n        cls_scores = self.cls_layer(rpn_feat)\n        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n\n        anchors = self.generate_anchors(image, feat)\n\n        number_of_anchors_per_location = cls_scores.size(1)\n        cls_scores = cls_scores.permute(0, 2, 3, 1)\n        cls_scores = cls_scores.reshape(-1, 1)\n\n        box_transform_pred = box_transform_pred.view(\n            box_transform_pred.size(0),\n            number_of_anchors_per_location,\n            4,\n            rpn_feat.shape[-2],\n            rpn_feat.shape[-1])\n        box_transform_pred = box_transform_pred.permute(0, 3, 4, 1, 2)\n        box_transform_pred = box_transform_pred.reshape(-1, 4)\n\n        proposals = apply_regression_pred_to_anchors_or_proposals(\n            box_transform_pred.detach().reshape(-1, 1, 4),\n            anchors)\n        proposals = proposals.reshape(proposals.size(0), 4)\n\n        proposals, scores = self.filter_proposals(proposals, cls_scores.detach(), image.shape)\n        rpn_output = {\n            'proposals': proposals,\n            'scores': scores\n        }\n        if not self.training or target is None:\n            return rpn_output\n        else:\n            labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_targets_to_anchors(\n                anchors,\n                target['bboxes'][0])\n\n            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_anchors, anchors)\n\n            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n                labels_for_anchors,\n                positive_count=self.rpn_pos_count,\n                total_count=self.rpn_batch_size)\n\n            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n\n            localization_loss = (\n                    torch.nn.functional.smooth_l1_loss(\n                        box_transform_pred[sampled_pos_idx_mask],\n                        regression_targets[sampled_pos_idx_mask],\n                        beta=1 / 9,\n                        reduction=\"sum\",\n                    )\n                    / (sampled_idxs.numel())\n            )\n\n            cls_loss = torch.nn.functional.binary_cross_entropy_with_logits(cls_scores[sampled_idxs].flatten(),\n                                                                            labels_for_anchors[sampled_idxs].flatten())\n\n            rpn_output['rpn_classification_loss'] = cls_loss\n            rpn_output['rpn_localization_loss'] = localization_loss\n            return rpn_output\n\n\nclass ROIHead(nn.Module):\n    r\"\"\"\n    ROI head on top of ROI pooling layer.\n    \"\"\"\n\n    def __init__(self, model_config, num_classes, in_channels):\n        super(ROIHead, self).__init__()\n        self.num_classes = num_classes\n        self.roi_batch_size = model_config['roi_batch_size']\n        self.roi_pos_count = int(model_config['roi_pos_fraction'] * self.roi_batch_size)\n        self.iou_threshold = model_config['roi_iou_threshold']\n        self.low_bg_iou = model_config['roi_low_bg_iou']\n        self.nms_threshold = model_config['roi_nms_threshold']\n        self.topK_detections = model_config['roi_topk_detections']\n        self.low_score_threshold = model_config['roi_score_threshold']\n        self.pool_size = model_config['roi_pool_size']\n        self.fc_inner_dim = model_config['fc_inner_dim']\n\n        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n        self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n\n        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n        torch.nn.init.constant_(self.cls_layer.bias, 0)\n\n        torch.nn.init.normal_(self.bbox_reg_layer.weight, std=0.001)\n        torch.nn.init.constant_(self.bbox_reg_layer.bias, 0)\n\n    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n        iou_matrix = get_iou(gt_boxes, proposals)\n        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n        background_proposals = (best_match_iou < self.iou_threshold) & (best_match_iou >= self.low_bg_iou)\n        ignored_proposals = best_match_iou < self.low_bg_iou\n\n        best_match_gt_idx[background_proposals] = -1\n        best_match_gt_idx[ignored_proposals] = -2\n\n        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n        labels = gt_labels[best_match_gt_idx.clamp(min=0)]\n        labels = labels.to(dtype=torch.int64)\n\n        labels[background_proposals] = 0\n        labels[ignored_proposals] = -1\n\n        return labels, matched_gt_boxes_for_proposals\n\n    def forward(self, feat, proposals, image_shape, target):\n        if self.training and target is not None:\n            proposals = torch.cat([proposals, target['bboxes'][0]], dim=0)\n\n            gt_boxes = target['bboxes'][0]\n            gt_labels = target['labels'][0]\n\n            labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(proposals, gt_boxes, gt_labels)\n\n            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(labels,\n                                                                                  positive_count=self.roi_pos_count,\n                                                                                  total_count=self.roi_batch_size)\n\n            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n\n            proposals = proposals[sampled_idxs]\n            labels = labels[sampled_idxs]\n            matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_proposals, proposals)\n\n        # ROI Pooling logic\n        size = feat.shape[-2:]\n        possible_scales = []\n        for s1, s2 in zip(size, image_shape):\n            approx_scale = float(s1) / float(s2)\n            scale = 2 ** float(torch.tensor(approx_scale).log2().round())\n            possible_scales.append(scale)\n        assert possible_scales[0] == possible_scales[1]\n\n        #\n        proposal_roi_pool_feats = torchvision.ops.roi_pool(feat, [proposals],\n                                                           output_size=self.pool_size,\n                                                           spatial_scale=possible_scales[0])\n        proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n        box_fc_6 = torch.nn.functional.relu(self.fc6(proposal_roi_pool_feats))\n        box_fc_7 = torch.nn.functional.relu(self.fc7(box_fc_6))\n        cls_scores = self.cls_layer(box_fc_7)\n        box_transform_pred = self.bbox_reg_layer(box_fc_7)\n\n        num_boxes, num_classes = cls_scores.shape\n        box_transform_pred = box_transform_pred.reshape(num_boxes, num_classes, 4)\n        frcnn_output = {}\n        if self.training and target is not None:\n            classification_loss = torch.nn.functional.cross_entropy(cls_scores, labels)\n\n            fg_proposals_idxs = torch.where(labels > 0)[0]\n            fg_cls_labels = labels[fg_proposals_idxs]\n\n            localization_loss = torch.nn.functional.smooth_l1_loss(\n                box_transform_pred[fg_proposals_idxs, fg_cls_labels],\n                regression_targets[fg_proposals_idxs],\n                beta=1/9,\n                reduction=\"sum\",\n            )\n            localization_loss = localization_loss / labels.numel()\n            frcnn_output['frcnn_classification_loss'] = classification_loss\n            frcnn_output['frcnn_localization_loss'] = localization_loss\n\n        if self.training:\n            return frcnn_output\n        else:\n            device = cls_scores.device\n            pred_boxes = apply_regression_pred_to_anchors_or_proposals(box_transform_pred, proposals)\n            pred_scores = torch.nn.functional.softmax(cls_scores, dim=-1)\n\n            pred_boxes = clamp_boxes_to_image_boundary(pred_boxes, image_shape)\n\n            pred_labels = torch.arange(num_classes, device=device)\n            pred_labels = pred_labels.view(1, -1).expand_as(pred_scores)\n\n            pred_boxes = pred_boxes[:, 1:]\n            pred_scores = pred_scores[:, 1:]\n            pred_labels = pred_labels[:, 1:]\n\n            pred_boxes = pred_boxes.reshape(-1, 4)\n            pred_scores = pred_scores.reshape(-1)\n            pred_labels = pred_labels.reshape(-1)\n\n            pred_boxes, pred_labels, pred_scores = self.filter_predictions(pred_boxes, pred_labels, pred_scores)\n            frcnn_output['boxes'] = pred_boxes\n            frcnn_output['scores'] = pred_scores\n            frcnn_output['labels'] = pred_labels\n            return frcnn_output\n\n    def filter_predictions(self, pred_boxes, pred_labels, pred_scores):\n        keep = torch.where(pred_scores > self.low_score_threshold)[0]\n        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n\n        min_size = 16\n        ws, hs = pred_boxes[:, 2] - pred_boxes[:, 0], pred_boxes[:, 3] - pred_boxes[:, 1]\n        keep = (ws >= min_size) & (hs >= min_size)\n        keep = torch.where(keep)[0]\n        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n\n        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n        for class_id in torch.unique(pred_labels):\n            curr_indices = torch.where(pred_labels == class_id)[0]\n            curr_keep_indices = torch.ops.torchvision.nms(pred_boxes[curr_indices],\n                                                          pred_scores[curr_indices],\n                                                          self.nms_threshold)\n            keep_mask[curr_indices[curr_keep_indices]] = True\n        keep_indices = torch.where(keep_mask)[0]\n        post_nms_keep_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n        keep = post_nms_keep_indices[:self.topK_detections]\n        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n        return pred_boxes, pred_labels, pred_scores\n\n\nclass FasterRCNN(nn.Module):\n    #\n    def __init__(self, model_config, num_classes):\n        super(FasterRCNN, self).__init__()\n        self.model_config = model_config\n        # Ensure we are not using pretrained weights for the 'from scratch' requirement\n        vgg16 = torchvision.models.vgg16(pretrained=False)\n        self.backbone = vgg16.features[:-1]\n        self.rpn = RegionProposalNetwork(model_config['backbone_out_channels'],\n                                         scales=model_config['scales'],\n                                         aspect_ratios=model_config['aspect_ratios'],\n                                         model_config=model_config)\n        self.roi_head = ROIHead(model_config, num_classes, in_channels=model_config['backbone_out_channels'])\n\n        # REMOVED: The freezing loop. For 'train from scratch', we must train the backbone.\n        # for layer in self.backbone[:10]:\n        #     for p in layer.parameters():\n        #         p.requires_grad = False\n\n        self.image_mean = [0.485, 0.456, 0.406]\n        self.image_std = [0.229, 0.224, 0.225]\n        self.min_size = model_config['min_im_size']\n        self.max_size = model_config['max_im_size']\n\n    def normalize_resize_image_and_boxes(self, image, bboxes):\n        dtype, device = image.dtype, image.device\n\n        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device)\n        std = torch.as_tensor(self.image_std, dtype=dtype, device=device)\n        image = (image - mean[:, None, None]) / std[:, None, None]\n\n        h, w = image.shape[-2:]\n        im_shape = torch.tensor(image.shape[-2:])\n        min_size = torch.min(im_shape).to(dtype=torch.float32)\n        max_size = torch.max(im_shape).to(dtype=torch.float32)\n        scale = torch.min(float(self.min_size) / min_size, float(self.max_size) / max_size)\n        scale_factor = scale.item()\n\n        image = torch.nn.functional.interpolate(\n            image,\n            size=None,\n            scale_factor=scale_factor,\n            mode=\"bilinear\",\n            recompute_scale_factor=True,\n            align_corners=False,\n        )\n\n        if bboxes is not None:\n            ratios = [\n                torch.tensor(s, dtype=torch.float32, device=bboxes.device)\n                / torch.tensor(s_orig, dtype=torch.float32, device=bboxes.device)\n                for s, s_orig in zip(image.shape[-2:], (h, w))\n            ]\n            ratio_height, ratio_width = ratios\n            xmin, ymin, xmax, ymax = bboxes.unbind(2)\n            xmin = xmin * ratio_width\n            xmax = xmax * ratio_width\n            ymin = ymin * ratio_height\n            ymax = ymax * ratio_height\n            bboxes = torch.stack((xmin, ymin, xmax, ymax), dim=2)\n        return image, bboxes\n\n    def forward(self, image, target=None):\n        old_shape = image.shape[-2:]\n        if self.training:\n            image, bboxes = self.normalize_resize_image_and_boxes(image, target['bboxes'])\n            target['bboxes'] = bboxes\n        else:\n            image, _ = self.normalize_resize_image_and_boxes(image, None)\n\n        feat = self.backbone(image)\n\n        rpn_output = self.rpn(image, feat, target)\n        proposals = rpn_output['proposals']\n\n        frcnn_output = self.roi_head(feat, proposals, image.shape[-2:], target)\n        if not self.training:\n            frcnn_output['boxes'] = transform_boxes_to_original_size(frcnn_output['boxes'],\n                                                                     image.shape[-2:],\n                                                                     old_shape)\n        return rpn_output, frcnn_output","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9byqcdNKaEOJ","outputId":"18683186-a2b5-43c3-ef32-486d5ac2a0b3","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:25:10.088368Z","iopub.execute_input":"2026-01-10T07:25:10.089054Z","iopub.status.idle":"2026-01-10T07:25:10.102702Z","shell.execute_reply.started":"2026-01-10T07:25:10.089031Z","shell.execute_reply":"2026-01-10T07:25:10.102040Z"}},"outputs":[{"name":"stdout","text":"Overwriting faster_rcnn.py\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%writefile infer.py\nimport torch\nimport numpy as np\nimport cv2\nimport argparse\nimport random\nimport os\nimport yaml\nimport time\nfrom tqdm import tqdm\nfrom faster_rcnn import FasterRCNN\nfrom voc import VOCDataset\nfrom torch.utils.data.dataloader import DataLoader\n\ndef get_device():\n    if torch.cuda.is_available(): return torch.device('cuda')\n    return torch.device('cpu')\n\ndevice = get_device()\n\ndef get_iou(det, gt):\n    det_x1, det_y1, det_x2, det_y2 = det\n    gt_x1, gt_y1, gt_x2, gt_y2 = gt\n    x_left = max(det_x1, gt_x1)\n    y_top = max(det_y1, gt_y1)\n    x_right = min(det_x2, gt_x2)\n    y_bottom = min(det_y2, gt_y2)\n    if x_right < x_left or y_bottom < y_top: return 0.0\n    area_intersection = (x_right - x_left) * (y_bottom - y_top)\n    det_area = (det_x2 - det_x1) * (det_y2 - det_y1)\n    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)\n    area_union = float(det_area + gt_area - area_intersection + 1E-6)\n    return area_intersection / area_union\n\ndef compute_map(det_boxes, gt_boxes, iou_threshold=0.5, method='interp'):\n    gt_labels = {cls_key for im_gt in gt_boxes for cls_key in im_gt.keys()}\n    gt_labels = sorted(gt_labels)\n    all_aps = {}\n    aps = []\n    for idx, label in enumerate(gt_labels):\n        cls_dets = [[im_idx, im_dets_label] for im_idx, im_dets in enumerate(det_boxes) if label in im_dets for im_dets_label in im_dets[label]]\n        cls_dets = sorted(cls_dets, key=lambda k: -k[1][-1])\n        gt_matched = [[False for _ in im_gts[label]] for im_gts in gt_boxes]\n        num_gts = sum([len(im_gts[label]) for im_gts in gt_boxes])\n        tp, fp = [0] * len(cls_dets), [0] * len(cls_dets)\n\n        for det_idx, (im_idx, det_pred) in enumerate(cls_dets):\n            im_gts = gt_boxes[im_idx][label]\n            max_iou_found, max_iou_gt_idx = -1, -1\n            for gt_box_idx, gt_box in enumerate(im_gts):\n                gt_box_iou = get_iou(det_pred[:-1], gt_box)\n                if gt_box_iou > max_iou_found:\n                    max_iou_found = gt_box_iou\n                    max_iou_gt_idx = gt_box_idx\n            if max_iou_found < iou_threshold or gt_matched[im_idx][max_iou_gt_idx]:\n                fp[det_idx] = 1\n            else:\n                tp[det_idx] = 1\n                gt_matched[im_idx][max_iou_gt_idx] = True\n\n        tp = np.cumsum(tp)\n        fp = np.cumsum(fp)\n        eps = np.finfo(np.float32).eps\n        recalls = tp / np.maximum(num_gts, eps)\n        precisions = tp / np.maximum((tp + fp), eps)\n\n        ap = 0.0\n        if method == 'interp':\n            for interp_pt in np.arange(0, 1 + 1E-3, 0.1):\n                prec_interp_pt = precisions[recalls >= interp_pt]\n                prec_interp_pt = prec_interp_pt.max() if prec_interp_pt.size > 0.0 else 0.0\n                ap += prec_interp_pt\n            ap = ap / 11.0\n\n        if num_gts > 0:\n            aps.append(ap)\n            all_aps[label] = ap\n        else:\n            all_aps[label] = np.nan\n    return sum(aps) / len(aps), all_aps\n\ndef print_model_metrics(model, device):\n    print(\"\\n\" + \"=\"*30)\n    print(\"MODEL PERFORMANCE METRICS\")\n    print(\"=\"*30)\n    total_params = sum(p.numel() for p in model.parameters())\n    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n    print(f\"Model Size (Weights): {(param_size + buffer_size) / 1024**2:.2f} MB\")\n    print(f\"Total Parameters: {total_params/1e6:.2f} Million\")\n\n    print(\"Calculating Inference FPS (Warmup + 50 runs)...\")\n    dummy_input = torch.randn(1, 3, 600, 600).to(device)\n    model.eval()\n    with torch.no_grad():\n        for _ in range(10): _ = model(dummy_input, None)\n        t0 = time.time()\n        for _ in range(50): _ = model(dummy_input, None)\n        t1 = time.time()\n    print(f\"Inference Speed: {50 / (t1 - t0):.2f} FPS\")\n    print(\"=\"*30 + \"\\n\")\n\ndef evaluate_map(args):\n    with open(args.config_path, 'r') as f: config = yaml.safe_load(f)\n    voc = VOCDataset('test', config['dataset_params']['im_test_path'], config['dataset_params']['ann_test_path'])\n    test_dataset = DataLoader(voc, batch_size=1, shuffle=False)\n\n    model = FasterRCNN(config['model_params'], num_classes=config['dataset_params']['num_classes'])\n    ckpt_path = config['train_params']['ckpt_name']\n    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n    model.to(device).eval()\n\n    print_model_metrics(model, device)\n\n    gts, preds = [], []\n    print(\"Starting mAP Evaluation...\")\n    with torch.no_grad():\n        for im, target, _ in tqdm(test_dataset):\n            im = im.float().to(device)\n            target_boxes = target['bboxes'].float().to(device)[0]\n            target_labels = target['labels'].long().to(device)[0]\n            _, frcnn_output = model(im, None)\n\n            pred_boxes_img, gt_boxes_img = {}, {}\n            for lbl in voc.label2idx: pred_boxes_img[lbl] = []; gt_boxes_img[lbl] = []\n\n            for idx, box in enumerate(frcnn_output['boxes']):\n                label_name = voc.idx2label[frcnn_output['labels'][idx].item()]\n                pred_boxes_img[label_name].append(frcnn_output['boxes'][idx].cpu().numpy().tolist() + [frcnn_output['scores'][idx].item()])\n\n            for idx, box in enumerate(target_boxes):\n                label_name = voc.idx2label[target_labels[idx].item()]\n                gt_boxes_img[label_name].append(box.cpu().numpy().tolist())\n\n            gts.append(gt_boxes_img)\n            preds.append(pred_boxes_img)\n\n    mean_ap, all_aps = compute_map(preds, gts)\n    print(f'\\nMean Average Precision : {mean_ap:.4f}')\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', dest='config_path', default='voc.yaml', type=str)\n    args = parser.parse_args()\n    evaluate_map(args)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qhBdPphlXg1c","outputId":"7f1ad13f-41aa-42db-9c6b-d4208f51d0cc","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:25:19.623695Z","iopub.execute_input":"2026-01-10T07:25:19.624344Z","iopub.status.idle":"2026-01-10T07:25:19.630724Z","shell.execute_reply.started":"2026-01-10T07:25:19.624319Z","shell.execute_reply":"2026-01-10T07:25:19.630061Z"}},"outputs":[{"name":"stdout","text":"Overwriting infer.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!PYTHONPATH=. python infer.py --config voc.yaml","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3PVQQREoXxG4","outputId":"79eadcf1-9c35-49b3-926b-3fa39709d65c","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:25:27.763103Z","iopub.execute_input":"2026-01-10T07:25:27.763348Z","iopub.status.idle":"2026-01-10T07:52:07.744200Z","shell.execute_reply.started":"2026-01-10T07:25:27.763330Z","shell.execute_reply":"2026-01-10T07:52:07.743510Z"}},"outputs":[{"name":"stdout","text":"{0: 'background', 1: 'aeroplane', 2: 'bicycle', 3: 'bird', 4: 'boat', 5: 'bottle', 6: 'bus', 7: 'car', 8: 'cat', 9: 'chair', 10: 'cow', 11: 'diningtable', 12: 'dog', 13: 'horse', 14: 'motorbike', 15: 'person', 16: 'pottedplant', 17: 'sheep', 18: 'sofa', 19: 'train', 20: 'tvmonitor'}\n100%|████████████████████████████████████| 17125/17125 [00:36<00:00, 468.14it/s]\nTotal 17125 images found\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n\n==============================\nMODEL PERFORMANCE METRICS\n==============================\nModel Size (Weights): 167.64 MB\nTotal Parameters: 43.95 Million\nCalculating Inference FPS (Warmup + 50 runs)...\nInference Speed: 15.03 FPS\n==============================\n\nStarting mAP Evaluation...\n100%|█████████████████████████████████████| 17125/17125 [25:47<00:00, 11.07it/s]\n\nMean Average Precision : 0.1677\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}